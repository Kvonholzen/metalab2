---
title: "Dynamical version of Promoting replicability in developmental research through meta-analyses: Insights from language acquisition research."
author: "Christina Bergmann"
date: "Last updated on `r Sys.Date()`."
---
  
# Introduction
```{r, include=FALSE}
#knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, include = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(metafor)
library(tidyverse)
library(pwr)
library(lme4)
library(kableExtra)
```


This document presents dynamically updating analyses that are based on the paper [Promoting Replicability in Developmental Research Through Meta-analyses: Insights From Language Acquisition Research](https://onlinelibrary.wiley.com/doi/full/10.1111/cdev.13079). This paper will always use the latest data from the language acquisition domain on [MetaLab](metalab.stanford.edu). 

```{r Preprocessing}
## CLEAN DATA ####
all_data = all_data %>%
  filter(domain == "early_language")%>%
  filter(is.na(condition_type) | condition_type == "critical") %>%
  filter(dataset!="Pointing and vocabulary (longitudinal)") %>%
  filter(infant_type == "typical") %>%
  mutate(weights_d = 1/(d_var_calc)^2) %>%
  mutate(d_var_calc = abs(d_var_calc))


all_data = all_data %>%
  mutate(year = as.numeric(unlist(lapply(strsplit(unlist(study_ID),
                                                  "[^0-9]+"),  function(x) unlist(x)[2])))) %>%
  mutate(year = ifelse(grepl("submitted",study_ID), 2017, year)) %>%
  mutate(year = ifelse(dataset == "Phonotactic learning", 
                       as.numeric(unlist(lapply(strsplit(unlist(short_cite),"[^0-9]+"),  function(x) unlist(x)[2]))), year)) %>%
  mutate(dataset = as.factor(dataset))

#Remove outliers

clean_data = all_data %>%
  group_by(dataset) %>%
  mutate(mean_es = median(d_calc)) %>%
  mutate(sd_es = sd(d_calc)) %>%
  ungroup() %>%
  mutate(no_outlier = ifelse(d_calc < mean_es+3*sd_es, ifelse(d_calc > mean_es-3*sd_es, TRUE, FALSE), FALSE))  %>%
  filter(no_outlier) 

#Comment out if you do not want to remove outliers
all_data = clean_data
remove(clean_data)

#Descriptives prep
get_power = function(df){
  pwr.t.test(n = df$n_dataset, d = df$d, sig.level = 0.05)
}

# Organize data to get descriptors
data_rma = all_data %>%
  nest(-dataset, .key = information) %>%
  mutate(model = map(information, ~rma.mv(d_calc, d_var_calc, random = ~ study_ID, data=.))) %>%
  mutate(d = map(model, "b")) %>%
  mutate(se = map(model, "se"))  %>%
  select(dataset, d, se) %>%
  mutate(d = as.numeric(as.character(d))) %>%
  mutate(se = as.numeric(as.character(se)))

# Descriptors
MA_descriptives = all_data %>%
  mutate(n_total = n) %>% #ifelse(!is.na(n_2), n_1 + n_2, n_1)) %>% I think n does the same thing
  group_by(dataset) %>%
  summarise(age_dataset = median(mean_age_months),
            age_min = min(mean_age_months),
            age_max = max(mean_age_months),
            n_dataset = median(n_total),
            n_min = min(n_total),
            n_max = max(n_total),
            n_records = n(),
            n_papers = length(unique(short_cite))) %>%
  ungroup() %>%
  inner_join(data_rma)

# Power
MA_power = MA_descriptives %>%
  nest(-dataset, .key = descriptives) %>%
  mutate(power = map(descriptives, get_power)) %>%
  mutate(power = map(power, "power")) %>%
  select(dataset, power) %>%
  mutate(power = as.numeric(as.character(power)))

# Summary
MA_summary = inner_join(MA_descriptives, MA_power) 

MA_summary_table = MA_summary %>%
  mutate(age = paste(as.character(round(age_dataset, 0)), " (", as.character(round(age_min, 0)), "-", as.character(round(age_max, 0)), ")", sep = "")) %>%
  mutate(n = paste(as.character(n_dataset), " (", as.character(n_min), "-", as.character(round(n_max, 0)), ")", sep = "")) %>%
  mutate(ES = paste(as.character(round(d, 2)), " (", as.character(round(se, 2)), ")", sep = "")) %>%
  select(dataset, age, n, n_records, n_papers, ES, power)


```



## Data


The data presented and analyzed here are part of a standardized collection of meta-analyses (MetaLab), and are freely available via the companion website at http://metalab.stanford.edu. Currently, MetaLab contains `r length(MA_descriptives$dataset)` meta-analyses, where core parts of each meta-analysis are standardized to allow for the computation of common effect size estimates and for analyses that span across different phenomena. These standardized variables include study descriptors (such as citation and peer review status), participant characteristics (including mean age and native language), methodological information (e.g., what dependent variable was measured), and information necessary to compute effect sizes (number of participants, if available means and standard deviations of the dependent measure, otherwise test statistics of the key hypothesis test, such as *t* values or *F* scores). Detailed descriptions of all phenomena covered by MetaLab, including which papers and other sources have been considered, can be found at http://metalab.stanford.edu.


# Results

## Sample size and statistical power

Table 1 provides a summary of typical sample sizes and effect sizes per meta-analysis. We remind the reader that recommendations are for power to be above 80%, which means that four out of five studies show a significant outcome for an effect truly present in the population.

As could be expected, sample sizes are small across all meta-analyses, with the overall median in our data being `r median(all_data$n)`  infants or paired observations (i.e. `r median(all_data$n)*2` participants in total in a between-participant design). Effect sizes predominantly fall into ranges of small to medium effects, as defined by Cohen [@cohen]. The overall median effect size of all data analyzed here is Cohen's *d* = `r median(abs(all_data$d_calc))`. As a result of those two factors, studies are typically severely under-powered. Assuming a paired t-test (within-participant designs are the most frequent in the present data), observed power is at `r round(pwr.t.test(n=median(all_data$n), d=median(abs(all_data$d_calc)), type = "paired")$power*100,  digits = 0)`% (for independent samples, observed power is at `r round(pwr.t.test(n=median(all_data$n), d=median(abs(all_data$d_calc)), type = "two.sample")$power*100,  digits = 0)`%).

With the observed sample size, it is possible to detect an effect in 80% of all studies when Cohen's *d* = `r round(pwr.t.test(n=median(all_data$n), d=NULL, power = .8, type = "paired")$d,  digits = 2)`; in other words, this sample size would be appropriate when investigating a medium to large effect. When comparing two independent groups, the effect size that would be detectable with a sample size of `r median(all_data$n)` participants per group increases to Cohen's *d* = `r round(pwr.t.test(n=median(all_data$n), d=NULL, power = .8, type = "two.sample")$d,  digits = 2)`, a large effect that is rarely observed as meta-analytic effect size in the present collection of developmental meta-analyses. 

Inversely, to detect the typical effect of Cohen's *d* = `r median(abs(all_data$d_calc))` with 80% power, studies would have to test `r round(pwr.t.test(n=NULL, d=median(abs(all_data$d_calc)), power = .8, type = "paired")$n,  digits = 0)` participants in a paired design; `r round(pwr.t.test(n=NULL, d=median(abs(all_data$d_calc)), power = .8, type = "paired")$n-median(all_data$n), digits = 0)` more than are included on average. 
For a between-participant design, a study with 80% power would require testing `r round(pwr.t.test(n=NULL, d=median(abs(all_data$d_calc)), power = .8, type = "two.sample")$n,  digits = 0)` infants per group, over four times the typical sample size we encounter here. 
This disparity between observed and necessary sample size varies greatly across meta-analyses, leading to drastic differences in observed power to detect the main effect. While studies on phonotactic learning and word segmentation apparently typically are dramatically underpowered (with observed power being under 10%), experiments on pointing and vocabulary, gaze following, and online word recognition are very well powered (92%, 95%, and 99%, respectively). 


```{r DescriptiveInformation, echo = FALSE, results='asis'}
get_power = function(df){
  pwr.t.test(n = df$n_dataset, d = df$d, sig.level = 0.05)
}

# Organize data to get descriptors
data_rma = all_data %>%
  nest(-dataset, .key = information) %>%
  mutate(model = map(information, ~rma.mv(d_calc, d_var_calc, random = ~ study_ID, data=.))) %>%
  mutate(d = map(model, "b")) %>%
  mutate(se = map(model, "se"))  %>%
  select(dataset, d, se) %>%
  mutate(d = as.numeric(as.character(d))) %>%
  mutate(se = as.numeric(as.character(se)))

# Descriptors
MA_descriptives = all_data %>%
  mutate(n_total = n) %>% #ifelse(!is.na(n_2), n_1 + n_2, n_1)) %>% I think n does the same thing
  group_by(dataset) %>%
  summarise(age_dataset = median(mean_age_months),
            age_min = min(mean_age_months),
            age_max = max(mean_age_months),
            n_dataset = median(n_total),
            n_min = min(n_total),
            n_max = max(n_total),
            n_records = n(),
            n_papers = length(unique(short_cite))) %>%
  ungroup() %>%
  inner_join(data_rma)

# Power
MA_power = MA_descriptives %>%
  nest(-dataset, .key = descriptives) %>%
  mutate(power = map(descriptives, get_power)) %>%
  mutate(power = map(power, "power")) %>%
  select(dataset, power) %>%
  mutate(power = as.numeric(as.character(power)))

# Summary
MA_summary = inner_join(MA_descriptives, MA_power) 

MA_summary_table = MA_summary %>%
  mutate(age = paste(as.character(round(age_dataset, 0)), " (", as.character(round(age_min, 0)), "-", as.character(round(age_max, 0)), ")", sep = "")) %>%
  mutate(n = paste(as.character(n_dataset), " (", as.character(n_min), "-", as.character(round(n_max, 0)), ")", sep = "")) %>%
  mutate(ES = paste(as.character(round(d, 2)), " (", as.character(round(se, 2)), ")", sep = "")) %>%
  select(dataset, age, n, n_records, n_papers, ES, power)


#MA_summary_table$power <- printnum(MA_summary_table$power)
kable(MA_summary_table, col.names = c("Meta-Analysis", "Age", "Sample Size", "N Effect Sizes", "N Papers", "Effect Size (SE)", "Power"), align = c("l", "r", "r", "r", "r", "r", "r"), caption = "Descriptions of meta-analyses. Age is reported in months, sample size is based on the median in a given meta-analysis, effect size is reported as meta-anlytic weighted median Cohen's d, and average power is computed based on meta-anlytic effect size estimate Cohen's d and median sample size.", digits = 3)
```


### Seminal papers as basis for sample size planning

As Table 1 shows, experimenters only rarely include a sufficient number of participants to observe a given effect -- assuming the meta-analytic estimate is accurate. It might, however, be possible, that power has been determined based on a seminal paper to be replicated and expanded. Initial reports tend to overestimate effect sizes , possibly explaining the lack of observed power in the subsequent literature. 

For each meta-analysis, we extracted the oldest paper and the largest effect size reported therein and re-calculated power accordingly, using the median sample size of the same meta-analysis (see Table 2). The largest effect size per paper was chosen because many seminal studies contain at least one null result in a control condition that delineates the limitations of a given phenomenon (for example that older children succeed at a task that their younger peers fail). Thus, it is unlikely that the researchers following up on that work aim for the median or mean effect size. 

In some cases, such as native and non-native vowel discrimination, as shown in Table 2, sample size choices match well with the oldest report. The difference in power, noted in the last column, can be substantial, with native vowel discrimination and phonotactic learning being the two most salient examples. Here, sample sizes match well with the oldest report and studies would be appropriately powered if this estimate were representative of the true effect. In four meta-analyses neither the seminal paper nor meta-analytic effect size seem to be a useful basis for sample size decisions. Since these numbers are based on the largest effect of a seminal paper, all power estimations (but also differences in meta-analytic effect sizes) would be smaller, meaning that sample sizes are less appropriate than implied by the column denoting power based on the seminal paper in Table 2.


```{r SeminalPaper, echo = FALSE, results='asis'}

get_power_oldest = function(df){
  pwr.t.test(n = df$n_dataset, d = df$largest_d, sig.level = 0.05)
}

# Compute oldest paper
oldest = all_data %>%
  group_by(dataset, short_cite) %>%
  summarise(year = max(year),
            largest_d = max(d_calc)) %>%
  ungroup() %>%
  group_by(dataset) %>%
  arrange(year) %>%
  filter(row_number() == 1) %>%
  ungroup()

# Combine summary with oldest paper
d_comparison = inner_join(oldest, MA_summary) %>%
  select(dataset, largest_d, d, n_dataset, power)

# Include power
d_comparison_power = d_comparison %>%
  nest(-dataset, .key = descriptives) %>%
  mutate(power = map(descriptives, get_power_oldest)) %>%
  mutate(old_power = map(power, "power")) %>%
  select(dataset, old_power) %>%
  mutate(old_power = as.numeric(as.character(old_power)))

# Save overall summary
d_comparison_summary = inner_join(d_comparison, d_comparison_power) %>%
  mutate(difference = old_power-as.numeric(power)) %>%
  select(-power)

kable(arrange(d_comparison_summary, old_power), col.names = c("Meta-Analysis", "Effect Size \n Seminal Paper", "Effect Size \n Overall", "Sample Size", "Power \n Seminal Paper", "Difference \n Overall Power"), align = c("l", "r", "r", "r", "r"), caption = "For each meta-analysis, largest effect size Cohen's d and derived power based on the seminal paper, along with the difference between power based on meta-analytic and seminal paper effect size.", digits = 3)
```


## Method choice



### Exclusion rates across methods 

```{r DropoutModel2, include=FALSE, results='asis'}

# this is a quick and dirty fix to get this file to work with the current system
# Kyle needs to come back later and fix this so it looks nice and pretty
# wkh May 24, 10`9

axislabels = c("CF", "CondHT", "FC", "HPP", "LwL", "SA")


## METHOD VS EXCLUDED ####

# Centering mean age
method_exclude_data = all_data %>%
  mutate(ageC = ifelse(participant_design == "between",
                       (((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2)/30.42,
                       (((mean_age_1 - mean(mean_age_1))))/30.42)) %>%
  mutate(keep = ifelse(is.na(n_2), n_1, n_1 + n_2)) %>%
  mutate(dropout = ifelse(is.na(n_excluded_1), NA, ifelse(is.na(n_excluded_2), n_excluded_1, n_excluded_1+n_excluded_2))) %>%
  mutate(total_run = keep + dropout) %>%
  filter(!is.na(dropout)) %>%
  mutate(percent_dropout = dropout*100 / total_run) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number > 10) %>%
  mutate(method = factor(method)) #%>%
  #select(percent_dropout, keep, dropout, total_run, dataset, ageC, method, mean_age_months)

method_data = all_data %>%
  filter(method %in% unique(method_exclude_data$method)) %>%
  mutate(ageC = ifelse(participant_design == "between",
                       (((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2)/30.42,
                       (((mean_age_1 - mean(mean_age_1))))/30.42)) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number > 10) %>%
  mutate(method = factor(method)) #%>%

# Build LMER model
method_exclude.m <- lmer(percent_dropout ~ method * ageC +
                           (1|dataset), data = method_exclude_data)


coefs.full <- as.data.frame(coef(summary(method_exclude.m)))
coefs.full$p <- format.pval(2*(1-pnorm(abs(coefs.full[,"t value"]))))



method_exclude_apa <- coefs.full


clean<-function(x){
  x<-gsub("relevel(, \"central fixation\")","",x,fixed=T)
  x<-gsub("intrcpt","Intercept",x,fixed=T)
  x<-gsub("(Intercept)","Intercept",x,fixed=T)
  x<-gsub("method","",x)
  x<-gsub("conditioned head-turn","CondHT",x)
  x<-gsub("forced-choice","FC",x)
  x<-gsub("head-turn preference procedure","HPP",x)
  x<-gsub("looking while listening","LwL",x)
  x<-gsub("stimulus alternation","SA",x)
  x<-gsub("ageC", "Age",x)
  x<-gsub(":", "*",x)}


rownames(method_exclude_apa)<-clean(rownames(method_exclude_apa))


colnames(method_exclude_apa)<-c("Est.", "SE Est", "t", "p")

#method_exclude_apa[,1] <- printnum(method_exclude_apa[,1])
#method_exclude_apa[,2] <- printnum(method_exclude_apa[,2])
#method_exclude_apa[,3] <- printnum(method_exclude_apa[,3])
kable(method_exclude_apa, caption = "Linear mixed effects model predicting exclusion rate by method and participant age while accounting for the specific phenomenon.", digits = 3)


```

In most of the analyzed meta-analyses, multiple methods were used to tap into the phenomenon in question. Choosing a robust method can help increase power, because more precise measurements lead to larger effect sizes due to reduced measurement variance and thus require fewer participants to be tested to conduct appropriately-powered studies. However, the number of participants relates to the final sample and not how many participants had to be invited into the lab. We thus first quantify whether methods differ in their typical exclusion rate, as economic considerations might drive method choice. To this end we consider all methods which have more than 10 associated effect sizes and for which information on the number of excluded participants was reported and entered in the meta-analyses. We note that this is exclusion rate, rather than fussout or dropout rates, because it represents the number excluded considering all criteria, including data quality criteria such as a minimum looking time. We chose this variable for practical reasons, as overall exclusion rates are more frequently reported than the number of participants who did not complete the experiment. The following analyses cover `r length(unique(method_exclude_data$method))` methods and `r length(method_exclude_data$method)` effect sizes.

The results of a linear mixed effects model predicting exclusion rate by method and mean participant age (while controlling for the different underlying effect sizes per meta-analysis) are summarized in Table 3 and visualized in Figure 1. The results show significant variation across methods, and a tendency toward higher exclusion rates for older participants, with some interaction with method.



```{r DropoutModel, echo = FALSE, results='asis'}
axislabels = c("CF", "CondHT", "FC", "HPP", "LwL", "SA")


## METHOD VS EXCLUDED ####

# Centering mean age
method_exclude_data = all_data %>%
  mutate(ageC = ifelse(participant_design == "between",
                       (((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2)/30.42,
                       (((mean_age_1 - mean(mean_age_1))))/30.42)) %>%
  mutate(keep = ifelse(is.na(n_2), n_1, n_1 + n_2)) %>%
  mutate(dropout = ifelse(is.na(n_excluded_1), NA, ifelse(is.na(n_excluded_2), n_excluded_1, n_excluded_1+n_excluded_2))) %>%
  mutate(total_run = keep + dropout) %>%
  filter(!is.na(dropout)) %>%
  mutate(percent_dropout = dropout*100 / total_run) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number > 10) %>%
  mutate(method = factor(method)) #%>%
  #select(percent_dropout, keep, dropout, total_run, dataset, ageC, method, mean_age_months)

method_data = all_data %>%
  filter(method %in% unique(method_exclude_data$method)) %>%
  mutate(ageC = ifelse(participant_design == "between",
                       (((mean_age_1 - mean(mean_age_1)) + (mean_age_1 - mean(mean_age_1)))/2)/30.42,
                       (((mean_age_1 - mean(mean_age_1))))/30.42)) %>%
  group_by(method) %>%
  mutate(number = n()) %>%
  ungroup() %>%
  filter(number > 10) %>%
  mutate(method = factor(method)) #%>%

# Build LMER model
method_exclude.m <- lmer(percent_dropout ~ method * ageC +
                           (1|dataset), data = method_exclude_data)


coefs.full <- as.data.frame(coef(summary(method_exclude.m)))
coefs.full$p <- format.pval(2*(1-pnorm(abs(coefs.full[,"t value"]))))



method_exclude_apa <- coefs.full


clean<-function(x){
  x<-gsub("relevel(, \"central fixation\")","",x,fixed=T)
  x<-gsub("intrcpt","Intercept",x,fixed=T)
  x<-gsub("(Intercept)","Intercept",x,fixed=T)
  x<-gsub("method","",x)
  x<-gsub("conditioned head-turn","CondHT",x)
  x<-gsub("forced-choice","FC",x)
  x<-gsub("head-turn preference procedure","HPP",x)
  x<-gsub("looking while listening","LwL",x)
  x<-gsub("stimulus alternation","SA",x)
  x<-gsub("ageC", "Age",x)
  x<-gsub(":", "*",x)}


rownames(method_exclude_apa)<-clean(rownames(method_exclude_apa))


colnames(method_exclude_apa)<-c("Est.", "SE Est", "t", "p")

#method_exclude_apa[,1] <- printnum(method_exclude_apa[,1])
#method_exclude_apa[,2] <- printnum(method_exclude_apa[,2])
#method_exclude_apa[,3] <- printnum(method_exclude_apa[,3])
kable(method_exclude_apa, caption = "Linear mixed effects model predicting exclusion rate by method and participant age while accounting for the specific phenomenon.", digits = 3)


```

```{r DropoutPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Exclusion rate in percent by different methods. CF = central fixation, CondHT = conditioned headturn, FC = forced choice, HPP = headturn preference procedure, LwL = looking while listening, SA = stimulus alternation. Each point indicates a single study."}
method_exclude.plot = ggplot(method_exclude_data, aes(x = method, y = percent_dropout)) +
  geom_boxplot() +
  geom_jitter(size = .5, alpha = .35) +
  xlab("Method") +
  ylab("Percent Excluded") +
  scale_x_discrete(labels = axislabels) +
  theme_classic() +
  theme(text = element_text(size=16), axis.line.x = element_line(), axis.line.y = element_line(), legend.position='none')
method_exclude.plot
```


### Effect sizes as a function of method

We built a meta-analytic model with Cohen's *d* as the dependent variable, and method and mean age centered as independent variables, which we allowed to interact. The model includes the variance of *d* for sampling variance, aa nested random effect of paper (inner random effect) within meta-analysis (outer random effect). We limited this analysis to the same methods that we investigated in the section on exclusion rates to be able to observe possible links between effect size and exclusion rate in methods. The model results in Table 4 show significant variation in effect sizes across methods, age, and some interaction of method and age.


```{r MethodEffect, echo = FALSE, results='asis'}
# Build model
method.rma <- rma.mv(d_calc, d_var_calc, mods = ~ageC * relevel(method, "central fixation"), random = ~ short_cite | dataset, data = method_data)

# Save summary of model
method.rma_sum = summary(method.rma)

method.rma_coef = coef(method.rma_sum)

method_rma_apa <- method.rma_coef

method_rma_apa=round(method_rma_apa,3)
method_rma_apa[,1]<-paste0(method_rma_apa[,1]," [",method_rma_apa[,5],",",method_rma_apa[,6],"]")
method_rma_apa=method_rma_apa[,1:4]
rownames(method_rma_apa)<-clean(rownames(method_rma_apa))

rownames(method_rma_apa)<-clean(rownames(method_rma_apa))

colnames(method_rma_apa)<-c("Est. (CI)","SE","z","p")


kable(method_rma_apa, caption ="Meta-analytic regression predicting effect size Cohen's d with participant age and method (central fixation is baseline method).", digits = 3)
```



```{r MethodPlot, echo = FALSE, ig.pos = "T!", fig.width=8, fig.height=5.5, fig.cap = "Effect size by different methods. CF = central fixation, CondHT = conditioned headturn, FC = forced choice, HPP = headturn preference procedure, LwL = looking while listening, SA = stimulus alternation. Each point indicates a single study."}
method.plot = ggplot(method_data, aes(x = method, y = d_calc)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_boxplot() +
  geom_jitter(size = .5, alpha = .35) +
  xlab("Method") +
  ylab("Effect size (Cohen's d)") +
  #xlim(0, 40) +
  ylim(-1.5, 3.3) +
  scale_x_discrete(labels = axislabels) +
  theme_classic() +
  theme(text = element_text(size=16), axis.line.x = element_line(), axis.line.y = element_line(), legend.position='none')


method.plot
```



## Questionable research practices

In the final set of analyses, we assess the relation between absolute observed effect sizes in single studies and the associated sample size. The rationale behind this analysis is simple: The smaller the effect size in a particular study (bear in mind that we assume that experiments sample from a distribution around the population effect), the larger the sample needed for a significant *p* value. If sample size decisions are made before data collection and all results are published, we expect no relation between observed effect size and sample size. If, on the contrary, authors continue to add infants to achieve significance, there should be a negative correlation between sample size and effect size. 


```{r Bias, echo = FALSE, ig.pos = "T!", fig.width=14, fig.height=14, fig.cap = "For each meta-analysis observed effect size per study plotted against sample size. Each point indicates a single study."}

bias_grid <-ggplot(all_data, aes(x = n, y = abs(d_calc))) +
  facet_wrap(~dataset, scales = "free",ncol = 3) +
  xlab("Sample size")  +
  ylab("Effect Size")  +
  geom_smooth(method = 'lm', se = T, colour = "darkgrey") +
  geom_point(size =.5, alpha = .75) +
  theme_classic() +
  theme(text = element_text(size=16), axis.line.x = element_line(), axis.line.y = element_line(), legend.position='none')


bias_grid
```

We illustrate the link between effect size and sample size, separated by meta-analysis, in Figure 3. The statistical test results for each meta-analysis can be found in Table 5. 



```{r BiasData, echo = FALSE, results='asis'}
data_bias = all_data %>%
  nest(-dataset, .key = information) %>%
  mutate(model = map(information, ~cor.test(abs(.$d_calc), .$n, method = "kendall"))) %>%
  mutate(p = map(model, "p.value")) %>%
  mutate(tau = map(model, "estimate"))  %>%
  select(dataset, tau, p) %>%
  unnest() %>%
  mutate(p = as.numeric(as.character(p))) %>%
  mutate(p = ifelse(p < .001, "< .001", as.character(round(p, 3)))) %>%
  mutate(tau = as.numeric(as.character(tau))) 


kable(data_bias, col.names = c("Meta-analysis", "Kendall's Tau", "p"), align = c("l", "r", "r"), caption = "Non-parametric correlations between sample sizes and effect sizes for each meta-analysis. A significant value indicates bias.", digits = 3)

```

